{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "992c99fb",
      "metadata": {
        "id": "992c99fb"
      },
      "source": [
        "# Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "FUCS9tuC_csH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCS9tuC_csH",
        "outputId": "b58ae997-bde6-4fab-8edc-0c836c241848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openml in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (0.12.2)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (2.5.0)\n",
            "Requirement already satisfied: minio in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (7.1.11)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (1.23.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (1.4.3)\n",
            "Requirement already satisfied: pyarrow in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: requests in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (2.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (1.9.0)\n",
            "Requirement already satisfied: xmltodict in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from openml) (0.13.0)\n",
            "Requirement already satisfied: certifi in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from minio->openml) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from minio->openml) (1.26.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from pandas>=1.0.0->openml) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from python-dateutil->openml) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from requests->openml) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from requests->openml) (2.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from scikit-learn>=0.18->openml) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/karma/Documents/AutoML Project/venv/lib/python3.8/site-packages (from scikit-learn>=0.18->openml) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78FOYnnJAhAs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78FOYnnJAhAs",
        "outputId": "63686fa3-1194-486d-b450-cbeb7264b200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "birds 40588\n",
            "(387, 260) (129, 260) (129, 260)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable\n",
        "\n",
        "import numpy as np\n",
        "import openml\n",
        "import pandas as pd\n",
        "from openml import OpenMLDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "task_ids = (\n",
        "    40588,\n",
        "    40589,\n",
        "    40590,\n",
        "    40591,\n",
        "    40592,\n",
        "    40593,\n",
        "    40594,\n",
        "    40595,\n",
        "    40596,\n",
        "    40597,\n",
        ")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    X: np.ndarray\n",
        "    y: np.ndarray\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Dataset:\n",
        "    name: str\n",
        "    id: int\n",
        "    features: pd.DataFrame\n",
        "    labels: pd.DataFrame\n",
        "    openml: OpenMLDataset\n",
        "    encoders: dict[str, LabelEncoder]\n",
        "\n",
        "    def split(\n",
        "        self,\n",
        "        splits: Iterable[float],\n",
        "        seed: int | None = 1,\n",
        "    ) -> tuple[Split, ...]:\n",
        "        \"\"\"Create splits of the data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        splits : Iterable[float]\n",
        "            The percentages of splits to generate\n",
        "\n",
        "        seed : int | None = None\n",
        "            The seed to use for the splits\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[Split, ...]\n",
        "            The collected splits\n",
        "        \"\"\"\n",
        "        splits = list(splits)\n",
        "        assert abs(1 - sum(splits)) <= 1e-6, \"Splits must sum to 1\"\n",
        "\n",
        "        sample_sizes = tuple(int(s * len(self.features)) for s in splits)\n",
        "\n",
        "        collected_splits = []\n",
        "\n",
        "        next_xs = self.features.to_numpy()\n",
        "        next_ys = self.labels.to_numpy()\n",
        "\n",
        "        for size in sample_sizes[:-1]:\n",
        "            xs, next_xs, ys, next_ys = train_test_split(\n",
        "                next_xs, next_ys, train_size=size, random_state=seed\n",
        "            )\n",
        "            collected_splits.append(Split(X=xs, y=ys))\n",
        "        collected_splits.append(Split(X=next_xs, y=next_ys))\n",
        "\n",
        "        return tuple(collected_splits)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_openml(id: int) -> Dataset:\n",
        "        \"\"\"Processes an multilabel OpenMLDataset into its features and targets\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        id: int\n",
        "            The id of the dataset\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Dataset\n",
        "        \"\"\"\n",
        "        dataset = openml.datasets.get_dataset(id)\n",
        "        print(dataset.name, id)\n",
        "        targets = dataset.default_target_attribute.split(\",\")\n",
        "        data, _, _, _ = dataset.get_data()\n",
        "\n",
        "        assert isinstance(data, pd.DataFrame)\n",
        "\n",
        "        # Process the features and turn all categorical columns into ints\n",
        "        features = data.drop(columns=targets)\n",
        "        encoders: dict[str, LabelEncoder] = {}\n",
        "\n",
        "        for name, col in features.iteritems():\n",
        "            if col.dtype in [\"object\", \"category\", \"string\"]:\n",
        "                encoder = LabelEncoder()\n",
        "                features[name] = encoder.fit_transform(col)\n",
        "                encoders[name] = encoder\n",
        "\n",
        "        labels = data[targets]\n",
        "\n",
        "        # Since we assume binary multilabel data, we convert the labels\n",
        "        # to all be boolean types\n",
        "        labels = labels.astype(bool)\n",
        "\n",
        "        return Dataset(\n",
        "            name=dataset.name,\n",
        "            id=id,\n",
        "            features=features,\n",
        "            labels=labels,\n",
        "            openml=dataset,\n",
        "            encoders=encoders,\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Open the first dataset in a browser\n",
        "    first = task_ids[0]\n",
        "    dataset = Dataset.from_openml(first)\n",
        "    dataset.openml.open_in_browser()\n",
        "\n",
        "    train, val, test = dataset.split(splits=(0.6, 0.2, 0.2))\n",
        "    print(train.X.shape, val.X.shape, test.X.shape)\n",
        "\n",
        "from sklearn.metrics import f1_score as sklearn_f1_score\n",
        "import numpy as np\n",
        "\n",
        "def f1_score(y_true: np.ndarray, y_pred:np.ndarray) -> float:\n",
        "    return sklearn_f1_score(y_true, y_pred, average=\"macro\", zero_division=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af08ed03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af08ed03",
        "outputId": "6897deb0-1fcf-4fac-9f68-59abae78999c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "birds 40588\n",
            "Training on birds\n",
            "Baseline score = 0.13217721723664\n",
            "emotions 40589\n",
            "Training on emotions\n",
            "Baseline score = 0.6246485178080441\n",
            "enron 40590\n",
            "Training on enron\n",
            "Baseline score = 0.15427559488991852\n",
            "genbase 40591\n",
            "Training on genbase\n",
            "Baseline score = 0.7160493827160493\n",
            "image 40592\n",
            "Training on image\n",
            "Baseline score = 0.4717961385901015\n",
            "langLog 40593\n",
            "Training on langLog\n",
            "Baseline score = 0.007548387096774194\n",
            "reuters 40594\n",
            "Training on reuters\n",
            "Baseline score = 0.5495908695133818\n",
            "scene 40595\n",
            "Training on scene\n",
            "Baseline score = 0.6922819362654442\n",
            "slashdot 40596\n",
            "Training on slashdot\n",
            "Baseline score = 0.2335916043894742\n",
            "yeast 40597\n",
            "Training on yeast\n",
            "Baseline score = 0.32679408884648337\n",
            "birds       0.132177\n",
            "emotions    0.624649\n",
            "enron       0.154276\n",
            "genbase     0.716049\n",
            "image       0.471796\n",
            "langLog     0.007548\n",
            "reuters     0.549591\n",
            "scene       0.692282\n",
            "slashdot    0.233592\n",
            "yeast       0.326794\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "class RandomForestBaseline:\n",
        "    def __init__(self, seed: int | None = 1):\n",
        "        self.seed = seed\n",
        "        self.estimator = RandomForestClassifier(random_state=seed)\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        self.estimator.fit(X, y)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return self.estimator.predict(X)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seed = 42\n",
        "    scores: dict[str, float] = {}\n",
        "\n",
        "    for id in task_ids:\n",
        "        dataset = Dataset.from_openml(id)\n",
        "        print(f\"Training on {dataset.name}\")\n",
        "\n",
        "        train, test = dataset.split(splits=(0.75, 0.25), seed=seed)\n",
        "        rf = RandomForestBaseline(seed=seed)\n",
        "\n",
        "        rf.fit(train.X, train.y)\n",
        "\n",
        "        predictions = rf.predict(test.X)\n",
        "        score = f1_score(test.y, predictions)\n",
        "        print(f\"Baseline score = {score}\")\n",
        "\n",
        "        scores[dataset.name] = score\n",
        "\n",
        "    results = pd.Series(scores)\n",
        "    print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704be117",
      "metadata": {
        "id": "704be117"
      },
      "source": [
        "# Neural Network multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fdd7b91e",
      "metadata": {
        "id": "fdd7b91e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/karma/Documents/AutoML Project/automl-multilabel/NN_Baseline_Pytorch.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/karma/Documents/AutoML%20Project/automl-multilabel/NN_Baseline_Pytorch.ipynb#ch0000005?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m make_multilabel_classification\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/karma/Documents/AutoML%20Project/automl-multilabel/NN_Baseline_Pytorch.ipynb#ch0000005?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m RepeatedKFold\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/karma/Documents/AutoML%20Project/automl-multilabel/NN_Baseline_Pytorch.ipynb#ch0000005?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/karma/Documents/AutoML%20Project/automl-multilabel/NN_Baseline_Pytorch.ipynb#ch0000005?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/karma/Documents/AutoML%20Project/automl-multilabel/NN_Baseline_Pytorch.ipynb#ch0000005?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n",
            "File \u001b[0;32m~/Documents/AutoML Project/venv/lib/python3.8/site-packages/keras/__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=2, random_state=1)\n",
        "\treturn X, y\n",
        "\n",
        "# get the model\n",
        "def get_model(n_inputs, n_outputs):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(20, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y):\n",
        "\tresults = list()\n",
        "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "\t# define evaluation procedure\n",
        "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# enumerate folds\n",
        "\tfor train_ix, test_ix in cv.split(X):\n",
        "\t\t# prepare data\n",
        "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
        "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
        "\t\t# define model\n",
        "\t\tmodel = get_model(n_inputs, n_outputs)\n",
        "\t\t# fit model\n",
        "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
        "\t\t# make a prediction on the test set\n",
        "\t\tyhat = model.predict(X_test)\n",
        "\t\t# round probabilities to class labels\n",
        "\t\tyhat = yhat.round()\n",
        "\t\t# calculate accuracy\n",
        "\t\tacc = f1_score(y_test, yhat)\n",
        "\t\t# store result\n",
        "\t\tprint('>%.3f' % acc)\n",
        "\t\tresults.append(acc)\n",
        "\treturn results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa52722",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "5fa52722",
        "outputId": "e8074bbc-5b1c-4ab2-bb5a-ebcbaaaaa996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "birds 40588\n",
            "Training on birds\n",
            "Baseline score = 0.04758107389686337\n",
            "emotions 40589\n",
            "Training on emotions\n",
            "Baseline score = 0.6430151745000587\n",
            "enron 40590\n",
            "Training on enron\n",
            "Baseline score = 0.18795163737722445\n",
            "genbase 40591\n",
            "Training on genbase\n",
            "Baseline score = 0.7138707334785765\n",
            "image 40592\n",
            "Training on image\n",
            "Baseline score = 0.4772824633980804\n",
            "langLog 40593\n",
            "Training on langLog\n",
            "Baseline score = 0.05308005673949399\n",
            "reuters 40594\n",
            "Training on reuters\n",
            "Baseline score = 0.6360635900215447\n",
            "scene 40595\n",
            "Training on scene\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3647c0f0ec71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# make a prediction on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "scores: dict[str, float] = {}\n",
        "\n",
        "for id in task_ids:\n",
        "    dataset = Dataset.from_openml(id)\n",
        "    print(f\"Training on {dataset.name}\")\n",
        "\n",
        "    train, test = dataset.split(splits=(0.75, 0.25), seed=seed)\n",
        "    n_inputs, n_outputs = len(dataset.features.columns), len(dataset.labels.columns)\n",
        "\n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    # fit model\n",
        "    model.fit(train.X,train.y, verbose=0, epochs=1000)\n",
        "    # make a prediction on the test set\n",
        "    yhat = model.predict(test.X)\n",
        "    # round probabilities to class labels\n",
        "    yhat = yhat.round()\n",
        "    # calculate accuracy\n",
        "    score = f1_score(test.y, yhat)\n",
        "    print(f\"Baseline score = {score}\")\n",
        "\n",
        "    scores[dataset.name] = score\n",
        "\n",
        "results = pd.Series(scores)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeLdgRQhwUHN",
      "metadata": {
        "id": "PeLdgRQhwUHN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "nyheV3T4wUQZ",
      "metadata": {
        "id": "nyheV3T4wUQZ"
      },
      "source": [
        "#Pytorch version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edWaUa8kwuEH",
      "metadata": {
        "id": "edWaUa8kwuEH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e-MluJ74wXC8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-MluJ74wXC8",
        "outputId": "ee0851ef-63a0-4d45-893e-c9fc2098af55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,input_dim,output_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20,output_dim),\n",
        "            nn.Sigmoid()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cxiXzs_415CK",
      "metadata": {
        "id": "cxiXzs_415CK"
      },
      "outputs": [],
      "source": [
        "from torch.functional import split\n",
        "class MyDataset(Dataset):\n",
        " \n",
        "  def __init__(self,split):\n",
        " \n",
        "    x=np.array(split.X)\n",
        "    y=np.array(split.y)\n",
        " \n",
        "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
        "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx],self.y_train[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GT0hxXsDwsuQ",
      "metadata": {
        "id": "GT0hxXsDwsuQ"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    #print(\"num batches: \"+str(num_batches))\n",
        "    #print(\"size: \"+str(size))\n",
        "    model.eval()\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            yhat = pred.round()\n",
        "            # calculate accuracy\n",
        "            f1 = f1_score(y.detach().cpu().numpy(), yhat.detach().cpu().numpy())\n",
        "    f1 /= num_batches\n",
        "    print(\"F1: \" + str(f1))\n",
        "    return f1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptJc3Tbj4iof",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptJc3Tbj4iof",
        "outputId": "51f95a48-6df1-4993-b460-85a63021ab9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "birds 40588\n",
            "Training on birds\n",
            "F1: 0.19133835385912948\n",
            "emotions 40589\n",
            "Training on emotions\n",
            "F1: 0.6669559412654514\n",
            "enron 40590\n",
            "Training on enron\n",
            "F1: 0.18228530642129812\n",
            "genbase 40591\n",
            "Training on genbase\n",
            "F1: 0.7242798353909465\n",
            "image 40592\n",
            "Training on image\n",
            "F1: 0.5384395657761082\n",
            "langLog 40593\n",
            "Training on langLog\n",
            "F1: 0.06936227481997208\n",
            "reuters 40594\n",
            "Training on reuters\n",
            "F1: 0.6474094064979523\n",
            "scene 40595\n",
            "Training on scene\n",
            "F1: 0.6965709355692465\n",
            "slashdot 40596\n",
            "Training on slashdot\n",
            "F1: 0.2960032043864161\n",
            "yeast 40597\n",
            "Training on yeast\n",
            "F1: 0.40822282176965224\n",
            "birds       0.191338\n",
            "emotions    0.666956\n",
            "enron       0.182285\n",
            "genbase     0.724280\n",
            "image       0.538440\n",
            "langLog     0.069362\n",
            "reuters     0.647409\n",
            "scene       0.696571\n",
            "slashdot    0.296003\n",
            "yeast       0.408223\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "scores: dict[str, float] = {}\n",
        "\n",
        "data = dict()\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "for id in task_ids:\n",
        "    dataset = Dataset.from_openml(id)\n",
        "    print(f\"Training on {dataset.name}\")\n",
        "\n",
        "    data[dataset.id] = dict()\n",
        "    data[dataset.id][\"dataset\"] = dataset.name\n",
        "\n",
        "    train_split, test_split = dataset.split(splits=(0.75, 0.25), seed=seed)\n",
        "    n_inputs, n_outputs = len(dataset.features.columns), len(dataset.labels.columns)\n",
        "    model = NeuralNetwork(n_inputs,n_outputs).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    train_dataset = MyDataset(train_split)\n",
        "    test_dataset = MyDataset(test_split)\n",
        "   \n",
        "    batch_size = 64\n",
        "\n",
        "    # Create data loaders.\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=test_dataset.__len__())\n",
        "\n",
        "    epochs = 1000\n",
        "    for t in range(epochs):\n",
        "        train(train_dataloader, model, loss_fn, optimizer)\n",
        "    score = test(test_dataloader,model, loss_fn)\n",
        "    scores[dataset.name] = score\n",
        "\n",
        "    data[dataset.id][\"score\"] = score\n",
        "\n",
        "results = pd.Series(scores)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K0_WbGW0ADgX",
      "metadata": {
        "id": "K0_WbGW0ADgX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NN Baseline Pytorch.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "9b72a21cbe115398aa6bac5fa154115bcc9847f9f8fc191c07ff58d228fc449c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
